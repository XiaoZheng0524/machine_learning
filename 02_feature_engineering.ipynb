{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 特征提取\n",
    "将任意数据（文本、图像等）转换为可用于机器学习的数字特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 字典特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " [[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  60.]\n",
      " [  0.   0.   1.  30.]] <class 'scipy.sparse._csr.csr_matrix'>\n",
      "特征名字：\n",
      " ['city=上海' 'city=北京' 'city=深圳' 'temperature']\n"
     ]
    }
   ],
   "source": [
    "data = [{'city': '北京','temperature':100}, \n",
    "        {'city': '上海','temperature':60}, \n",
    "        {'city': '深圳','temperature':30}]\n",
    "\n",
    "# 1、实例化一个转换器类\n",
    "transfer = DictVectorizer(sparse=True)\n",
    "\n",
    "# 2、调用fit_transform()\n",
    "data_new = transfer.fit_transform(data)\n",
    "print(\"data_new:\\n\", data_new.toarray(), type(data_new))\n",
    "print(\"特征名字：\\n\", transfer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 文本特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 CountVectorizer: 统计词频, 忽略单个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " [[0 1 1 0 1 1]\n",
      " [1 1 0 1 1 0]]\n",
      "特征名字：\n",
      " ['dislike' 'life' 'like' 'long' 'python' 'short']\n"
     ]
    }
   ],
   "source": [
    "data = [\"Life is short,I like python\", \"Life is too long,I dislike python\"]\n",
    "\n",
    "# 1、实例化一个转换器类, stop_words为停用词，剔除一些无用词\n",
    "transfer = CountVectorizer(stop_words=[\"is\", \"too\"])\n",
    "\n",
    "# 2、调用fit_transform\n",
    "data_new = transfer.fit_transform(data)\n",
    "print(\"data_new:\\n\", data_new.toarray())\n",
    "print(\"特征名字：\\n\", transfer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " [[0 1]\n",
      " [1 0]]\n",
      "特征名字：\n",
      " ['天安门上太阳升' '我爱北京天安门']\n"
     ]
    }
   ],
   "source": [
    "# 中文文本数据如果不采用空格或符号分开，会将一整个句子当作一个字\n",
    "data = [\"我爱北京天安门\", \"天安门上太阳升\"]\n",
    "# 1、实例化一个转换器类\n",
    "transfer = CountVectorizer()\n",
    "\n",
    "# 2、调用fit_transform\n",
    "data_new = transfer.fit_transform(data)\n",
    "print(\"data_new:\\n\", data_new.toarray())\n",
    "print(\"特征名字：\\n\", transfer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " [[1 1 0]\n",
      " [0 1 1]]\n",
      "特征名字：\n",
      " ['北京' '天安门' '太阳']\n"
     ]
    }
   ],
   "source": [
    "# 需要对中文句子进行分词处理\n",
    "data = [\"我 爱 北京 天安门\", \"天安门 上 太阳 升\"]\n",
    "# 1、实例化一个转换器类\n",
    "transfer = CountVectorizer()\n",
    "\n",
    "# 2、调用fit_transform\n",
    "data_new = transfer.fit_transform(data)\n",
    "print(\"data_new:\\n\", data_new.toarray())\n",
    "print(\"特征名字：\\n\", transfer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.435 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " [[0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0]\n",
      " [0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1]\n",
      " [1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]]\n",
      "特征名字：\n",
      " ['不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个' '看到'\n",
      " '真正' '秘密' '绝对' '美好' '联系' '过去' '还是' '这样']\n"
     ]
    }
   ],
   "source": [
    "# 使用jieba自动分词\n",
    "import jieba\n",
    "data = [\"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\",\n",
    "        \"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "        \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\"]\n",
    "\n",
    "# 1、分词\n",
    "data_new = []\n",
    "for i in range(len(data)):\n",
    "    data_new.append(' '.join(list(jieba.cut(data[i]))))\n",
    "\n",
    "transfer = CountVectorizer(stop_words=[\"一种\", \"所以\"])\n",
    "\n",
    "# 2、调用fit_transform\n",
    "data_final = transfer.fit_transform(data_new)\n",
    "print(\"data_new:\\n\", data_final.toarray())\n",
    "print(\"特征名字：\\n\", transfer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 TfidfVectorizer: \n",
    "- tf: term frequency,词频=某个词在该文本中出现的次数/该文本中所有词的总数\n",
    "- idf: inverse document frequency,逆向文档频率=log10(总文件数目/包含该词语的文件数)\n",
    "- tf*idf: 希望词在一个类别文章中出现的频率很高，在其他类别的文章中那个出现的频率很低,以便于得到更具有代表性的关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " [[0.         0.21821789 0.         0.         0.         0.43643578\n",
      "  0.         0.         0.         0.         0.         0.21821789\n",
      "  0.         0.21821789 0.         0.         0.         0.\n",
      "  0.21821789 0.         0.43643578 0.         0.21821789 0.\n",
      "  0.43643578 0.21821789 0.         0.         0.         0.21821789\n",
      "  0.21821789 0.         0.         0.21821789 0.        ]\n",
      " [0.         0.         0.2410822  0.         0.         0.\n",
      "  0.2410822  0.2410822  0.2410822  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2410822  0.55004769\n",
      "  0.         0.         0.         0.2410822  0.         0.\n",
      "  0.         0.         0.48216441 0.         0.         0.\n",
      "  0.         0.         0.2410822  0.         0.2410822 ]\n",
      " [0.15895379 0.         0.         0.63581516 0.47686137 0.\n",
      "  0.         0.         0.         0.15895379 0.15895379 0.\n",
      "  0.15895379 0.         0.15895379 0.15895379 0.         0.12088845\n",
      "  0.         0.15895379 0.         0.         0.         0.15895379\n",
      "  0.         0.         0.         0.31790758 0.15895379 0.\n",
      "  0.         0.15895379 0.         0.         0.        ]]\n",
      "特征名字：\n",
      " ['不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个' '看到'\n",
      " '真正' '秘密' '绝对' '美好' '联系' '过去' '还是' '这样']\n"
     ]
    }
   ],
   "source": [
    "data = [\"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\",\n",
    "        \"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "        \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\"]\n",
    "\n",
    "# 1、分词\n",
    "data_new = []\n",
    "for i in range(len(data)):\n",
    "    data_new.append(' '.join(list(jieba.cut(data[i]))))\n",
    "\n",
    "transfer = TfidfVectorizer(stop_words=[\"一种\", \"所以\"])\n",
    "\n",
    "# 2、调用fit_transform\n",
    "data_final = transfer.fit_transform(data_new)\n",
    "print(\"data_new:\\n\", data_final.toarray())\n",
    "print(\"特征名字：\\n\", transfer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征预处理\n",
    "无量纲化处理，避免一些特征数值过大使得模型无法对其他特征进行有效的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 归一化\n",
    "归一化的鲁棒性较差，容易受数据中出现异常点的影响，适合处理规模较小的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "      milage     Liters  Consumtime\n",
      "0     40920   8.326976    0.953952\n",
      "1     14488   7.153469    1.673904\n",
      "2     26052   1.441871    0.805124\n",
      "3     75136  13.147394    0.428964\n",
      "4     38344   1.669788    0.134296\n",
      "..      ...        ...         ...\n",
      "995   11145   3.410627    0.631838\n",
      "996   68846   9.974715    0.669787\n",
      "997   26575  10.650102    0.866627\n",
      "998   48111   9.134528    0.728045\n",
      "999   43757   7.882601    1.332446\n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "data_new:\n",
      " [[2.44832535 2.39805139 2.56233353]\n",
      " [2.15873259 2.34195467 2.98724416]\n",
      " [2.28542943 2.06892523 2.47449629]\n",
      " ...\n",
      " [2.29115949 2.50910294 2.51079493]\n",
      " [2.52711097 2.43665451 2.4290048 ]\n",
      " [2.47940793 2.3768091  2.78571804]]\n"
     ]
    }
   ],
   "source": [
    "# 1、获取数据\n",
    "data = pd.read_csv(\"data/dating.txt\")\n",
    "data = data.iloc[:, :3]\n",
    "print(\"data:\\n\", data)\n",
    "\n",
    "# 2、实例化一个转换器类,将特征值放缩到feature_range\n",
    "transfer = MinMaxScaler(feature_range=(2, 3))\n",
    "\n",
    "# 3、调用fit_transform\n",
    "data_new = transfer.fit_transform(data)\n",
    "print(\"data_new:\\n\", data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 标准化\n",
    "在样本较多的情况下比较稳定，适合处理现代嘈杂大数据场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "      milage     Liters  Consumtime\n",
      "0     40920   8.326976    0.953952\n",
      "1     14488   7.153469    1.673904\n",
      "2     26052   1.441871    0.805124\n",
      "3     75136  13.147394    0.428964\n",
      "4     38344   1.669788    0.134296\n",
      "..      ...        ...         ...\n",
      "995   11145   3.410627    0.631838\n",
      "996   68846   9.974715    0.669787\n",
      "997   26575  10.650102    0.866627\n",
      "998   48111   9.134528    0.728045\n",
      "999   43757   7.882601    1.332446\n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "data_new:\n",
      " [[ 0.33193158  0.41660188  0.24523407]\n",
      " [-0.87247784  0.13992897  1.69385734]\n",
      " [-0.34554872 -1.20667094 -0.05422437]\n",
      " ...\n",
      " [-0.32171752  0.96431572  0.06952649]\n",
      " [ 0.65959911  0.60699509 -0.20931587]\n",
      " [ 0.46120328  0.31183342  1.00680598]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/dating.txt\")\n",
    "data = data.iloc[:, :3]\n",
    "print(\"data:\\n\", data)\n",
    "\n",
    "# 2、实例化一个转换器类\n",
    "transfer = StandardScaler()\n",
    "\n",
    "# 3、调用fit_transform\n",
    "data_new = transfer.fit_transform(data)\n",
    "print(\"data_new:\\n\", data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征降维\n",
    "在某些限定条件下，降低随机变量特征的个数，得到一组不相关的主变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
